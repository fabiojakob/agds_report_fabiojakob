---
title: "Report Exercise Chapter 10"
author: "Ziggy Stardust"
output:
  html_document:
    toc: true
---
Comparison of the linear regression and the KNN models
1.

```{r}
library(readr)
library(lubridate)
library(dplyr)
daily_fluxes <- read_csv("C:/Users/fabio.LAPTOP-FR4VQFHF/Documents/agds_report_fabiojakob/Data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv") |>  
  
  # select only the variables we are interested in
  dplyr::select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all all meteorological covariates
                -contains("JSB")   # weird useless variable
                ) |>

  # convert to a nice date object
  dplyr::mutate(TIMESTAMP = ymd(TIMESTAMP)) |>

  # set all -9999 to NA
  mutate(across(where(is.numeric), ~na_if(., -9999))) |>   # NOTE: Newer tidyverse version no longer support this statement
                         # instead, use `mutate(across(where(is.numeric), ~na_if(., -9999))) |> `
  
  # retain only data based on >=80% good-quality measurements
  # overwrite bad data with NA (not dropping rows)
  dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF),
                TA_F           = ifelse(TA_F_QC        < 0.8, NA, TA_F),
                SW_IN_F        = ifelse(SW_IN_F_QC     < 0.8, NA, SW_IN_F),
                LW_IN_F        = ifelse(LW_IN_F_QC     < 0.8, NA, LW_IN_F),
                VPD_F          = ifelse(VPD_F_QC       < 0.8, NA, VPD_F),
                PA_F           = ifelse(PA_F_QC        < 0.8, NA, PA_F),
                P_F            = ifelse(P_F_QC         < 0.8, NA, P_F),
                WS_F           = ifelse(WS_F_QC        < 0.8, NA, WS_F)) |> 

  # drop QC variables (no longer needed)
  dplyr::select(-ends_with("_QC"))
```
```{r}
caret::train(
  form = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
  data = daily_fluxes |> drop_na(),  # drop missing values
  trControl = caret::trainControl(method = "none"),  # no resampling
  method = "lm"
)
```
```{r}
library(caret)
caret::train(
  form = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
  data = daily_fluxes |> drop_na(), 
  trControl = caret::trainControl(method = "none"),
  method = "knn"
)
```
```{r}
set.seed(123)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)
```

```{r}
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = daily_fluxes_train) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())
```
```{r}
caret::train(
  pp, 
  data = daily_fluxes_train, 
  method = "knn",
  trControl = caret::trainControl(method = "none")
)

```
```{r}
daily_fluxes |> 
  summarise(across(where(is.numeric), ~quantile(.x, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE))) |> 
  t() |> 
  as_tibble(rownames = "variable") |> 
  setNames(c("variable", "min", "q25", "q50", "q75", "max"))
```
```{r}
pp_prep <- recipes::prep(pp, training = daily_fluxes_train) 
```
```{r}
daily_fluxes_juiced <- recipes::juice(pp_prep)
```
```{r}
daily_fluxes_baked <- recipes::bake(pp_prep, new_data = daily_fluxes_train)

# confirm that juice and bake return identical objects when given the same data
all_equal(daily_fluxes_juiced, daily_fluxes_baked)
```
```{r}

visdat::vis_miss(
  daily_fluxes,
  cluster = FALSE, 
  warn_large_data = FALSE
  )
```
```{r}
pp |> 
  step_impute_median(all_predictors())
```
```{r}
pp |> 
  step_impute_knn(all_predictors(), neighbors = 5)
```
```{r}
# original data frame
df <- tibble(id = 1:4, color = c("red", "red", "green", "blue"))
df
```
```{r}
library(caret)
# after one-hot encoding
dmy <- dummyVars("~ .", data = df, sep = "_")
data.frame(predict(dmy, newdata = df))
```
```{r}
recipe(GPP_NT_VUT_REF ~ ., data = daily_fluxes) |> 
  step_dummy(all_nominal(), one_hot = TRUE)
```

```{r}
caret::nearZeroVar(daily_fluxes, saveMetrics = TRUE)
```
```{r}
pp |> 
  step_zv(all_predictors())
```


```{r}
recipes::recipe(WS_F ~ ., data = daily_fluxes) |>   # it's of course non-sense to model wind speed like this
  recipes::step_log(all_outcomes())
```

```{r}
library(tidyr)
pp <- recipe(WS_F ~ ., data = daily_fluxes_train) |>
  step_BoxCox(all_outcomes())
```

```{r}
prep_pp <- prep(pp, training = daily_fluxes_train |> drop_na())
daily_fluxes_baked <- bake(prep_pp, new_data = daily_fluxes_test |> drop_na())
daily_fluxes_baked |>
  ggplot(aes(x = WS_F, y = ..density..)) +
  geom_histogram() +
  labs(title = "Box-Cox-transformed")
```


```{r}
recipe(WS_F ~ ., data = daily_fluxes) |>
  step_YeoJohnson(all_outcomes())
```


```{r}
library(tidyr)
library(rsample)
library(recipes)
# Data splitting
set.seed(1982)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

# Model and pre-processing formulation, use all variables but LW_IN_F
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = daily_fluxes_train |> drop_na()) |> 
  recipes::step_BoxCox(all_predictors()) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())

# Fit linear regression model
mod_lm <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "lm",
  trControl = caret::trainControl(method = "none"),
  metric = "RMSE"
)

# Fit KNN model
mod_knn <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = data.frame(k = 8),
  metric = "RMSE"
)
```
```{r}
source("C:/Users/fabio.LAPTOP-FR4VQFHF/Documents/agds_report_fabiojakob/vignettes/re_ml_01_eval_model.r")
```

```{r}
library(tidyr)
library(ggplot2)
# linear regression model
eval_model(mod = mod_lm, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```


```{r}
library(ggplot2)
library(tidyr)
# KNN
eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```
2. Interpret observed differences in the context of the bias-variance trade-off:

-   Why is the difference between the evaluation on the training and the test set larger for the KNN model     than the linear regression model? 

  The difference between the evaluation on the training and test set is larger for the KNN model than for   the linear regression model because the KNN model has a higher variance than the linear regression        model. This means that the KNN model is more sensitive to the training data and may overfit to the        training data. In contrast, the linear regression model has lower variance and is less sensitive to       the training data.
  
-   Why is the does the evaluation on the test set indicate a better model performance of the KNN model       than the linear regression model?

  The evaluation on the test set indicates a better performance of the KNN model than the linear            regression model because the KNN model has a lower bias than the linear regression model. This means      that the KNN model can capture more complex patterns in the data that the linear regression model may     miss due to its assumption of a linear relationship between the variables. The KNN model may be better    suited to capture non-linear relationships in the data.
  
-   How would you position the KNN and the linear regression model along the spectrum of the bias-variance     trade-off?

  Along the spectrum of the bias-variance trade-off, the KNN model would be positioned closer to the high   variance end, whereas the linear regression model would be positioned closer to the high bias end. The    KNN model has lower bias and higher variance, which makes it more flexible and better at capturing        complex patterns in the data but also more prone to overfitting. The linear regression model has higher   bias and lower variance, which makes it less flexible but more robust to noise and less prone to          overfitting. The choice of model will depend on the specific requirements of the problem and the          trade-off between bias and variance that is acceptable for the given application.
  
3.
```{r}
# Load necessary packages
library(ggplot2)

daily_fluxes$predicted_values_lm <- predict(mod_lm, newdata = daily_fluxes)

# Define a function to create a line plot of observed and modelled GPP for a given model
plot_gpp <- function(model_name, df_obs, df_mod) {
  
  # Subset data frames to include only GPP and date columns
  df_obs <- df_obs[, c("TIMESTAMP", "GPP_NT_VUT_REF")]
  df_mod <- df_mod[, c("TIMESTAMP", "fitted")]
  
  # Rename columns in modelled GPP data frame to match observed GPP data frame
  colnames(df_mod) <- c("DATE", "GPP_NT_VUT_REF")
  
  # Combine observed and modelled GPP data frames
  df_all <- rbind(df_obs, df_mod)
  
  # Create plot
  plot <- ggplot(df_all, aes(x = DATE, y = GPP_NT_VUT_REF, color = model_name)) +
    geom_line() +
    labs(title = "Observed and Modelled GPP",
         subtitle = model_name,
         x = "Date",
         y = "GPP (umol m^-2 s^-1)",
         color = "Model") +
    theme_bw()
  
  return(plot)
}

# Call the plot_gpp function for each model
plot1 <- plot_gpp("Model 1", daily_fluxes, df_mod1)
plot2 <- plot_gpp("Model 2", daily_fluxes, df_mod2)

# Arrange plots side by side using the patchwork package
library(patchwork)
plot1 + plot2

```

