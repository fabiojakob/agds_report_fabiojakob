---
title: "Report Exercise Chapter 10"
author: "Ziggy Stardust"
output:
  html_document:
    toc: true
---
Comparison of the linear regression and the KNN models
1.

```{r}
library(readr)
library(lubridate)
library(dplyr)
daily_fluxes <- read_csv("C:/Users/fabio.LAPTOP-FR4VQFHF/Documents/agds_report_fabiojakob/Data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv") |>  
  
  # select only the variables we are interested in
  dplyr::select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all all meteorological covariates
                -contains("JSB")   # weird useless variable
                ) |>

  # convert to a nice date object
  dplyr::mutate(TIMESTAMP = ymd(TIMESTAMP)) |>

  # set all -9999 to NA
  mutate(across(where(is.numeric), ~na_if(., -9999))) |>   # NOTE: Newer tidyverse version no longer support this statement
                         # instead, use `mutate(across(where(is.numeric), ~na_if(., -9999))) |> `
  
  # retain only data based on >=80% good-quality measurements
  # overwrite bad data with NA (not dropping rows)
  dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF),
                TA_F           = ifelse(TA_F_QC        < 0.8, NA, TA_F),
                SW_IN_F        = ifelse(SW_IN_F_QC     < 0.8, NA, SW_IN_F),
                LW_IN_F        = ifelse(LW_IN_F_QC     < 0.8, NA, LW_IN_F),
                VPD_F          = ifelse(VPD_F_QC       < 0.8, NA, VPD_F),
                PA_F           = ifelse(PA_F_QC        < 0.8, NA, PA_F),
                P_F            = ifelse(P_F_QC         < 0.8, NA, P_F),
                WS_F           = ifelse(WS_F_QC        < 0.8, NA, WS_F)) |> 

  # drop QC variables (no longer needed)
  dplyr::select(-ends_with("_QC"))
```
```{r}
library(tidyverse)
caret::train(
  form = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
  data = daily_fluxes |> drop_na(),  # drop missing values
  trControl = caret::trainControl(method = "none"),  # no resampling
  method = "lm"
)
```
```{r}
library(caret)
caret::train(
  form = GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
  data = daily_fluxes |> drop_na(), 
  trControl = caret::trainControl(method = "none"),
  method = "knn"
)
```
```{r}
set.seed(123)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)
```

```{r}
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = daily_fluxes_train) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())
```

```{r}
caret::train(
  pp, 
  data = daily_fluxes_train, 
  method = "knn",
  trControl = caret::trainControl(method = "none")
)

```
```{r}
daily_fluxes |> 
  summarise(across(where(is.numeric), ~quantile(.x, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE))) |> 
  t() |> 
  as_tibble(rownames = "variable") |> 
  setNames(c("variable", "min", "q25", "q50", "q75", "max"))
```
```{r}
pp_prep <- recipes::prep(pp, training = daily_fluxes_train) 
```
```{r}
daily_fluxes_juiced <- recipes::juice(pp_prep)
```
```{r}
daily_fluxes_baked <- recipes::bake(pp_prep, new_data = daily_fluxes_train)

# confirm that juice and bake return identical objects when given the same data
all_equal(daily_fluxes_juiced, daily_fluxes_baked)
```
```{r}

visdat::vis_miss(
  daily_fluxes,
  cluster = FALSE, 
  warn_large_data = FALSE
  )
```
```{r}
pp |> 
  step_impute_median(all_predictors())
```
```{r}
pp |> 
  step_impute_knn(all_predictors(), neighbors = 5)
```
```{r}
# original data frame
df <- tibble(id = 1:4, color = c("red", "red", "green", "blue"))
df
```
```{r}
library(caret)
# after one-hot encoding
dmy <- dummyVars("~ .", data = df, sep = "_")
data.frame(predict(dmy, newdata = df))
```
```{r}
recipe(GPP_NT_VUT_REF ~ ., data = daily_fluxes) |> 
  step_dummy(all_nominal(), one_hot = TRUE)
```

```{r}
caret::nearZeroVar(daily_fluxes, saveMetrics = TRUE)
```
```{r}
pp |> 
  step_zv(all_predictors())
```


```{r}
recipes::recipe(WS_F ~ ., data = daily_fluxes) |>   # it's of course non-sense to model wind speed like this
  recipes::step_log(all_outcomes())
```

```{r}
library(tidyr)
pp <- recipe(WS_F ~ ., data = daily_fluxes_train) |>
  step_BoxCox(all_outcomes())
```

```{r}
prep_pp <- prep(pp, training = daily_fluxes_train |> drop_na())
daily_fluxes_baked <- bake(prep_pp, new_data = daily_fluxes_test |> drop_na())
daily_fluxes_baked |>
  ggplot(aes(x = WS_F, y = ..density..)) +
  geom_histogram() +
  labs(title = "Box-Cox-transformed")
```


```{r}
recipe(WS_F ~ ., data = daily_fluxes) |>
  step_YeoJohnson(all_outcomes())
```


```{r}
library(tidyr)
library(rsample)
library(recipes)
# Data splitting
set.seed(1982)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

# Model and pre-processing formulation, use all variables but LW_IN_F
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = daily_fluxes_train |> drop_na()) |> 
  recipes::step_BoxCox(all_predictors()) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())

# Fit linear regression model
mod_lm <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "lm",
  trControl = caret::trainControl(method = "none"),
  metric = "RMSE"
)

# Fit KNN model
mod_knn <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = data.frame(k = 8),
  metric = "RMSE"
)
```
```{r}
source("C:/Users/fabio.LAPTOP-FR4VQFHF/Documents/agds_report_fabiojakob/vignettes/re_ml_01_eval_model.r")
```

```{r}
library(tidyr)
library(ggplot2)
# linear regression model
eval_model(mod = mod_lm, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```


```{r}
library(ggplot2)
library(tidyr)
# KNN
eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```
2. Interpret observed differences in the context of the bias-variance trade-off:

-   Why is the difference between the evaluation on the training and the test set larger for the KNN model ,than the linear regression model? 

The difference between the train and test set scores is larger for the ANN model than for the linear regression model because the ANN model has more variance than the linear regression model. This means that the ANN model is more sensitive to the training data and maybe overfits the training data. In contrast, linear regression models have less variance and are less sensitive to the training data.
  
-   Why is the does the evaluation on the test set indicate a better model performance of the KNN model than the linear regression model?

The evaluation on the test set indicates a better performance of the KNN model than the linear regression model because the KNN model has a lower bias than the linear regression model. This means that the KNN model can capture more complex patterns in the data that the linear regression model maybe miss. The KNN model may be better suited to capture non-linear
relationships in the data.
  
-   How would you position the KNN and the linear regression model along the spectrum of the bias-variance trade-off?

Along the spectrum of the bias-variance trade-off, the KNN model would be positioned closer to the high variance end, whereas the linear regression model would be positioned closer to the high bias end. The KNN model has lower bias and higher variance. The linear regression model has higher bias and lower variance, which makes it less flexible but more robust to noise and less prone to overfitting. The choice of model will depend on the specific requirements of the problem and the trade-off between bias and variance that is acceptable for the given application.
  
3.
```{r}
library(ggplot2)
daily_fluxes <- na.omit(daily_fluxes)

# Create a data frame with observed and predicted GPP for linear regression
linear_regression_predictions <- data.frame(
  Date = daily_fluxes$TIMESTAMP,
  Observed = daily_fluxes$GPP_NT_VUT_REF,
  Predicted = predict(mod_lm, newdata = daily_fluxes |> drop_na())
)

# Create a data frame with observed and predicted GPP for KNN
knn_predictions <- data.frame(
  Date = daily_fluxes$TIMESTAMP,
  Observed = daily_fluxes$GPP_NT_VUT_REF,
  Predicted = predict(mod_knn, newdata = daily_fluxes |> drop_na())
)


ggplot(linear_regression_predictions, aes(x = Date)) +
  geom_line(aes(y = Observed, color = "Observed")) +
  geom_line(aes(y = Predicted, color = "Linear Regression Predicted")) +
  labs(x = "Date", y = "GPP", title = "Observed vs. Linear Regression Predicted GPP") +
  scale_color_manual(values = c("Observed" = "black", "Linear Regression Predicted" = "blue"))


ggplot(knn_predictions, aes(x = Date)) +
  geom_line(aes(y = Observed, color = "Observed")) +
  geom_line(aes(y = Predicted, color = "KNN Predicted")) +
  labs(x = "Date", y = "GPP", title = "Observed vs. KNN Predicted GPP") +
  scale_color_manual(values = c("Observed" = "black", "KNN Predicted" = "red"))

```
The role of k:

1. Hypothesis for k approaching 1:
   - R2: R2 on the training set would increase, while R2 on the test set would decrease due to overfitting.
   - MAE: MAE on the training set would decrease, while MAE on the test set would increase due to increased sensitivity to      noise and outliers.

2. Hypothesis for k approaching N:
   - R2: R2 on both the training and test sets would decrease due to a more generalized model.
   - MAE: MAE on the training set would increase, while MAE on the test set would decrease slightly or remain stable due to      smoothing of predictions.


```{r}
library(rsample)
library(caret)
library(tidyr)
library(ggplot2)

# Function to fit KNN model and return MAE on test set
calculate_MAE <- function(k) {
  set.seed(1982) 
  split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
  daily_fluxes_train <- rsample::training(split)
  daily_fluxes_test <- rsample::testing(split)
  
  
  pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                        data = daily_fluxes_train |> drop_na()) |> 
    recipes::step_BoxCox(all_predictors()) |> 
    recipes::step_center(all_numeric(), -all_outcomes()) |>
    recipes::step_scale(all_numeric(), -all_outcomes())
  
  # Fit KNN model
  mod_knn <- caret::train(
    pp, 
    data = daily_fluxes_train |> drop_na(), 
    method = "knn",
    trControl = caret::trainControl(method = "none"),
    tuneGrid = data.frame(k = k),
    metric = "MAE"
  )
  
  # Predict on test set
  test_predictions <- predict(mod_knn, newdata = daily_fluxes_test |> drop_na())
  
  # Calculate MAE on test set
  test_mae <- caret::MAE(test_predictions, daily_fluxes_test$GPP_NT_VUT_REF |> na.omit())
  
  return(test_mae)
}

# Range of k values to test
k_values <- seq(1, 50, by = 1)

# Calculate MAE for different k values
mae_results <- sapply(k_values, calculate_MAE)

# Exercise 3: Find optimal k (minimum MAE)
optimal_k <- k_values[which.min(mae_results)]
optimal_mae <- min(mae_results)

# Viualization
data <- data.frame(k = k_values, MAE = mae_results)
ggplot(data, aes(x = k, y = MAE)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = optimal_k, linetype = "dashed", color = "red") +
  labs(x = "k", y = "MAE", title = "MAE (Predictive Accuracy) vs. k (Complexity") +
  annotate("text", x = optimal_k + 2, y = optimal_mae + 0.1, label = paste("Optimal k =", optimal_k), color = "red")


```


